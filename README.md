# micrograd-mlp-from-scratch
A fully connected multi-layer perceptron (MLP) implemented from scratch in Python using a custom autograd engine (Value class). This project demonstrates forward pass, backward pass, and gradient descent without using PyTorch or TensorFlow.

Features

Custom autograd engine (Value class) for automatic differentiation

Neuron, Layer, and MLP classes implemented from scratch

Supports tanh activation

Training on a toy dataset using mean squared error (MSE) loss

Gradient descent optimizer implemented manually
